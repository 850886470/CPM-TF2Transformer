{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = torch.load('./model-v1/80000/mp_rank_00_model_states.pt', map_location='cpu')\n",
    "m1 = torch.load('./model-v1/80000/mp_rank_01_model_states.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(model, name):\n",
    "    for n, w in model['module'].items():\n",
    "        if name == n:\n",
    "            return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "word_embeddings.weight torch.Size([15000, 2560])\nposition_embeddings.weight torch.Size([1024, 2560])\ntransformer.layers.0.input_layernorm.weight torch.Size([2560])\ntransformer.layers.0.input_layernorm.bias torch.Size([2560])\ntransformer.layers.0.attention.query_key_value.weight torch.Size([3840, 2560])\ntransformer.layers.0.attention.query_key_value.bias torch.Size([3840])\ntransformer.layers.0.attention.dense.weight torch.Size([2560, 1280])\ntransformer.layers.0.attention.dense.bias torch.Size([2560])\ntransformer.layers.0.post_attention_layernorm.weight torch.Size([2560])\ntransformer.layers.0.post_attention_layernorm.bias torch.Size([2560])\ntransformer.layers.0.mlp.dense_h_to_4h.weight torch.Size([5120, 2560])\ntransformer.layers.0.mlp.dense_h_to_4h.bias torch.Size([5120])\ntransformer.layers.0.mlp.dense_4h_to_h.weight torch.Size([2560, 5120])\ntransformer.layers.0.mlp.dense_4h_to_h.bias torch.Size([2560])\ntransformer.final_layernorm.weight torch.Size([2560])\ntransformer.final_layernorm.bias torch.Size([2560])\n"
     ]
    }
   ],
   "source": [
    "for n, w in m0['module'].items():\n",
    "    if '.layers.' in n:\n",
    "        if '.layers.0.' in n:\n",
    "            print(n, w.shape)\n",
    "    else:\n",
    "        print(n, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a5ff64c3b5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, TFGPT2LMHeadModel\n",
    "\n",
    "gpt2_config = GPT2Config(\n",
    "    vocab_size=30000,\n",
    "    n_positions=1024,\n",
    "    n_ctx=1024,\n",
    "    n_embd=2560,\n",
    "    n_layer=32,\n",
    "    n_head=32,\n",
    "    pad_token_id=0,\n",
    ")\n",
    "gpt2_model = TFGPT2LMHeadModel(gpt2_config)\n",
    "loss = gpt2_model.compute_loss\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# metric = Mymetrice('accuracy')\n",
    "\n",
    "gpt2_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[loss, *[None] * gpt2_model.config.n_layer],\n",
    "    metrics=[metric]\n",
    ")\n",
    "input = tf.constant([[1, 2]])\n",
    "out = gpt2_model(input)[0]\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tfgp_t2lm_head_model/transformer/wte/weight:0 (30000, 2560)\ntfgp_t2lm_head_model/transformer/wpe/embeddings:0 (1024, 2560)\ntfgp_t2lm_head_model/transformer/h_._0/ln_1/gamma:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._0/ln_1/beta:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._0/attn/c_attn/weight:0 (2560, 7680)\ntfgp_t2lm_head_model/transformer/h_._0/attn/c_attn/bias:0 (1, 7680)\ntfgp_t2lm_head_model/transformer/h_._0/attn/c_proj/weight:0 (2560, 2560)\ntfgp_t2lm_head_model/transformer/h_._0/attn/c_proj/bias:0 (1, 2560)\ntfgp_t2lm_head_model/transformer/h_._0/ln_2/gamma:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._0/ln_2/beta:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._0/mlp/c_fc/weight:0 (2560, 10240)\ntfgp_t2lm_head_model/transformer/h_._0/mlp/c_fc/bias:0 (1, 10240)\ntfgp_t2lm_head_model/transformer/h_._0/mlp/c_proj/weight:0 (10240, 2560)\ntfgp_t2lm_head_model/transformer/h_._0/mlp/c_proj/bias:0 (1, 2560)\ntfgp_t2lm_head_model/transformer/h_._31/ln_1/gamma:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._31/ln_1/beta:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._31/attn/c_attn/weight:0 (2560, 7680)\ntfgp_t2lm_head_model/transformer/h_._31/attn/c_attn/bias:0 (1, 7680)\ntfgp_t2lm_head_model/transformer/h_._31/attn/c_proj/weight:0 (2560, 2560)\ntfgp_t2lm_head_model/transformer/h_._31/attn/c_proj/bias:0 (1, 2560)\ntfgp_t2lm_head_model/transformer/h_._31/ln_2/gamma:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._31/ln_2/beta:0 (2560,)\ntfgp_t2lm_head_model/transformer/h_._31/mlp/c_fc/weight:0 (2560, 10240)\ntfgp_t2lm_head_model/transformer/h_._31/mlp/c_fc/bias:0 (1, 10240)\ntfgp_t2lm_head_model/transformer/h_._31/mlp/c_proj/weight:0 (10240, 2560)\ntfgp_t2lm_head_model/transformer/h_._31/mlp/c_proj/bias:0 (1, 2560)\ntfgp_t2lm_head_model/transformer/ln_f/gamma:0 (2560,)\ntfgp_t2lm_head_model/transformer/ln_f/beta:0 (2560,)\n"
     ]
    }
   ],
   "source": [
    "for w in gpt2_model.weights:\n",
    "    if 'h_._' in w.name:\n",
    "        if 'h_._0' in w.name or 'h_._31' in w.name:\n",
    "            print(w.name, w.shape)\n",
    "    else:\n",
    "        print(w.name, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3840, 2560]) torch.Size([3840, 2560])\n(2560, 7680)\n"
     ]
    }
   ],
   "source": [
    "w0 = find_weight(m0, 'transformer.layers.0.attention.query_key_value.weight')\n",
    "w1 = find_weight(m1, 'transformer.layers.0.attention.query_key_value.weight')\n",
    "print(w0.shape, w1.shape)\n",
    "w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "w = np.transpose(w)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.00921,  0.01455,  0.04315, ...,  0.01168, -0.00566, -0.02007],\n",
       "      dtype=float16)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "setting transformer/wte/weight\n",
      "setting transformer/wpe/embeddings\n",
      "setting layer 0 ln_1/gamma\n",
      "setting layer 0 ln_1/beta\n",
      "setting layer 0 c_attn/weight\n",
      "setting layer 0 c_attn/bias\n",
      "setting layer 0 c_proj/weight\n",
      "setting layer 0 c_proj/bias\n",
      "setting layer 0 ln_2/gamma\n",
      "setting layer 0 ln_2/beta\n",
      "setting layer 0 mlp/c_fc/weight\n",
      "setting layer 0 mlp/c_fc/bias\n",
      "setting layer 0 mlp/c_proj/weight\n",
      "setting layer 0 mlp/c_proj/bias\n",
      "setting layer 1 ln_1/gamma\n",
      "setting layer 1 ln_1/beta\n",
      "setting layer 1 c_attn/weight\n",
      "setting layer 1 c_attn/bias\n",
      "setting layer 1 c_proj/weight\n",
      "setting layer 1 c_proj/bias\n",
      "setting layer 1 ln_2/gamma\n",
      "setting layer 1 ln_2/beta\n",
      "setting layer 1 mlp/c_fc/weight\n",
      "setting layer 1 mlp/c_fc/bias\n",
      "setting layer 1 mlp/c_proj/weight\n",
      "setting layer 1 mlp/c_proj/bias\n",
      "setting layer 2 ln_1/gamma\n",
      "setting layer 2 ln_1/beta\n",
      "setting layer 2 c_attn/weight\n",
      "setting layer 2 c_attn/bias\n",
      "setting layer 2 c_proj/weight\n",
      "setting layer 2 c_proj/bias\n",
      "setting layer 2 ln_2/gamma\n",
      "setting layer 2 ln_2/beta\n",
      "setting layer 2 mlp/c_fc/weight\n",
      "setting layer 2 mlp/c_fc/bias\n",
      "setting layer 2 mlp/c_proj/weight\n",
      "setting layer 2 mlp/c_proj/bias\n",
      "setting layer 3 ln_1/gamma\n",
      "setting layer 3 ln_1/beta\n",
      "setting layer 3 c_attn/weight\n",
      "setting layer 3 c_attn/bias\n",
      "setting layer 3 c_proj/weight\n",
      "setting layer 3 c_proj/bias\n",
      "setting layer 3 ln_2/gamma\n",
      "setting layer 3 ln_2/beta\n",
      "setting layer 3 mlp/c_fc/weight\n",
      "setting layer 3 mlp/c_fc/bias\n",
      "setting layer 3 mlp/c_proj/weight\n",
      "setting layer 3 mlp/c_proj/bias\n",
      "setting layer 4 ln_1/gamma\n",
      "setting layer 4 ln_1/beta\n",
      "setting layer 4 c_attn/weight\n",
      "setting layer 4 c_attn/bias\n",
      "setting layer 4 c_proj/weight\n",
      "setting layer 4 c_proj/bias\n",
      "setting layer 4 ln_2/gamma\n",
      "setting layer 4 ln_2/beta\n",
      "setting layer 4 mlp/c_fc/weight\n",
      "setting layer 4 mlp/c_fc/bias\n",
      "setting layer 4 mlp/c_proj/weight\n",
      "setting layer 4 mlp/c_proj/bias\n",
      "setting layer 5 ln_1/gamma\n",
      "setting layer 5 ln_1/beta\n",
      "setting layer 5 c_attn/weight\n",
      "setting layer 5 c_attn/bias\n",
      "setting layer 5 c_proj/weight\n",
      "setting layer 5 c_proj/bias\n",
      "setting layer 5 ln_2/gamma\n",
      "setting layer 5 ln_2/beta\n",
      "setting layer 5 mlp/c_fc/weight\n",
      "setting layer 5 mlp/c_fc/bias\n",
      "setting layer 5 mlp/c_proj/weight\n",
      "setting layer 5 mlp/c_proj/bias\n",
      "setting layer 6 ln_1/gamma\n",
      "setting layer 6 ln_1/beta\n",
      "setting layer 6 c_attn/weight\n",
      "setting layer 6 c_attn/bias\n",
      "setting layer 6 c_proj/weight\n",
      "setting layer 6 c_proj/bias\n",
      "setting layer 6 ln_2/gamma\n",
      "setting layer 6 ln_2/beta\n",
      "setting layer 6 mlp/c_fc/weight\n",
      "setting layer 6 mlp/c_fc/bias\n",
      "setting layer 6 mlp/c_proj/weight\n",
      "setting layer 6 mlp/c_proj/bias\n",
      "setting layer 7 ln_1/gamma\n",
      "setting layer 7 ln_1/beta\n",
      "setting layer 7 c_attn/weight\n",
      "setting layer 7 c_attn/bias\n",
      "setting layer 7 c_proj/weight\n",
      "setting layer 7 c_proj/bias\n",
      "setting layer 7 ln_2/gamma\n",
      "setting layer 7 ln_2/beta\n",
      "setting layer 7 mlp/c_fc/weight\n",
      "setting layer 7 mlp/c_fc/bias\n",
      "setting layer 7 mlp/c_proj/weight\n",
      "setting layer 7 mlp/c_proj/bias\n",
      "setting layer 8 ln_1/gamma\n",
      "setting layer 8 ln_1/beta\n",
      "setting layer 8 c_attn/weight\n",
      "setting layer 8 c_attn/bias\n",
      "setting layer 8 c_proj/weight\n",
      "setting layer 8 c_proj/bias\n",
      "setting layer 8 ln_2/gamma\n",
      "setting layer 8 ln_2/beta\n",
      "setting layer 8 mlp/c_fc/weight\n",
      "setting layer 8 mlp/c_fc/bias\n",
      "setting layer 8 mlp/c_proj/weight\n",
      "setting layer 8 mlp/c_proj/bias\n",
      "setting layer 9 ln_1/gamma\n",
      "setting layer 9 ln_1/beta\n",
      "setting layer 9 c_attn/weight\n",
      "setting layer 9 c_attn/bias\n",
      "setting layer 9 c_proj/weight\n",
      "setting layer 9 c_proj/bias\n",
      "setting layer 9 ln_2/gamma\n",
      "setting layer 9 ln_2/beta\n",
      "setting layer 9 mlp/c_fc/weight\n",
      "setting layer 9 mlp/c_fc/bias\n",
      "setting layer 9 mlp/c_proj/weight\n",
      "setting layer 9 mlp/c_proj/bias\n",
      "setting layer 10 ln_1/gamma\n",
      "setting layer 10 ln_1/beta\n",
      "setting layer 10 c_attn/weight\n",
      "setting layer 10 c_attn/bias\n",
      "setting layer 10 c_proj/weight\n",
      "setting layer 10 c_proj/bias\n",
      "setting layer 10 ln_2/gamma\n",
      "setting layer 10 ln_2/beta\n",
      "setting layer 10 mlp/c_fc/weight\n",
      "setting layer 10 mlp/c_fc/bias\n",
      "setting layer 10 mlp/c_proj/weight\n",
      "setting layer 10 mlp/c_proj/bias\n",
      "setting layer 11 ln_1/gamma\n",
      "setting layer 11 ln_1/beta\n",
      "setting layer 11 c_attn/weight\n",
      "setting layer 11 c_attn/bias\n",
      "setting layer 11 c_proj/weight\n",
      "setting layer 11 c_proj/bias\n",
      "setting layer 11 ln_2/gamma\n",
      "setting layer 11 ln_2/beta\n",
      "setting layer 11 mlp/c_fc/weight\n",
      "setting layer 11 mlp/c_fc/bias\n",
      "setting layer 11 mlp/c_proj/weight\n",
      "setting layer 11 mlp/c_proj/bias\n",
      "setting layer 12 ln_1/gamma\n",
      "setting layer 12 ln_1/beta\n",
      "setting layer 12 c_attn/weight\n",
      "setting layer 12 c_attn/bias\n",
      "setting layer 12 c_proj/weight\n",
      "setting layer 12 c_proj/bias\n",
      "setting layer 12 ln_2/gamma\n",
      "setting layer 12 ln_2/beta\n",
      "setting layer 12 mlp/c_fc/weight\n",
      "setting layer 12 mlp/c_fc/bias\n",
      "setting layer 12 mlp/c_proj/weight\n",
      "setting layer 12 mlp/c_proj/bias\n",
      "setting layer 13 ln_1/gamma\n",
      "setting layer 13 ln_1/beta\n",
      "setting layer 13 c_attn/weight\n",
      "setting layer 13 c_attn/bias\n",
      "setting layer 13 c_proj/weight\n",
      "setting layer 13 c_proj/bias\n",
      "setting layer 13 ln_2/gamma\n",
      "setting layer 13 ln_2/beta\n",
      "setting layer 13 mlp/c_fc/weight\n",
      "setting layer 13 mlp/c_fc/bias\n",
      "setting layer 13 mlp/c_proj/weight\n",
      "setting layer 13 mlp/c_proj/bias\n",
      "setting layer 14 ln_1/gamma\n",
      "setting layer 14 ln_1/beta\n",
      "setting layer 14 c_attn/weight\n",
      "setting layer 14 c_attn/bias\n",
      "setting layer 14 c_proj/weight\n",
      "setting layer 14 c_proj/bias\n",
      "setting layer 14 ln_2/gamma\n",
      "setting layer 14 ln_2/beta\n",
      "setting layer 14 mlp/c_fc/weight\n",
      "setting layer 14 mlp/c_fc/bias\n",
      "setting layer 14 mlp/c_proj/weight\n",
      "setting layer 14 mlp/c_proj/bias\n",
      "setting layer 15 ln_1/gamma\n",
      "setting layer 15 ln_1/beta\n",
      "setting layer 15 c_attn/weight\n",
      "setting layer 15 c_attn/bias\n",
      "setting layer 15 c_proj/weight\n",
      "setting layer 15 c_proj/bias\n",
      "setting layer 15 ln_2/gamma\n",
      "setting layer 15 ln_2/beta\n",
      "setting layer 15 mlp/c_fc/weight\n",
      "setting layer 15 mlp/c_fc/bias\n",
      "setting layer 15 mlp/c_proj/weight\n",
      "setting layer 15 mlp/c_proj/bias\n",
      "setting layer 16 ln_1/gamma\n",
      "setting layer 16 ln_1/beta\n",
      "setting layer 16 c_attn/weight\n",
      "setting layer 16 c_attn/bias\n",
      "setting layer 16 c_proj/weight\n",
      "setting layer 16 c_proj/bias\n",
      "setting layer 16 ln_2/gamma\n",
      "setting layer 16 ln_2/beta\n",
      "setting layer 16 mlp/c_fc/weight\n",
      "setting layer 16 mlp/c_fc/bias\n",
      "setting layer 16 mlp/c_proj/weight\n",
      "setting layer 16 mlp/c_proj/bias\n",
      "setting layer 17 ln_1/gamma\n",
      "setting layer 17 ln_1/beta\n",
      "setting layer 17 c_attn/weight\n",
      "setting layer 17 c_attn/bias\n",
      "setting layer 17 c_proj/weight\n",
      "setting layer 17 c_proj/bias\n",
      "setting layer 17 ln_2/gamma\n",
      "setting layer 17 ln_2/beta\n",
      "setting layer 17 mlp/c_fc/weight\n",
      "setting layer 17 mlp/c_fc/bias\n",
      "setting layer 17 mlp/c_proj/weight\n",
      "setting layer 17 mlp/c_proj/bias\n",
      "setting layer 18 ln_1/gamma\n",
      "setting layer 18 ln_1/beta\n",
      "setting layer 18 c_attn/weight\n",
      "setting layer 18 c_attn/bias\n",
      "setting layer 18 c_proj/weight\n",
      "setting layer 18 c_proj/bias\n",
      "setting layer 18 ln_2/gamma\n",
      "setting layer 18 ln_2/beta\n",
      "setting layer 18 mlp/c_fc/weight\n",
      "setting layer 18 mlp/c_fc/bias\n",
      "setting layer 18 mlp/c_proj/weight\n",
      "setting layer 18 mlp/c_proj/bias\n",
      "setting layer 19 ln_1/gamma\n",
      "setting layer 19 ln_1/beta\n",
      "setting layer 19 c_attn/weight\n",
      "setting layer 19 c_attn/bias\n",
      "setting layer 19 c_proj/weight\n",
      "setting layer 19 c_proj/bias\n",
      "setting layer 19 ln_2/gamma\n",
      "setting layer 19 ln_2/beta\n",
      "setting layer 19 mlp/c_fc/weight\n",
      "setting layer 19 mlp/c_fc/bias\n",
      "setting layer 19 mlp/c_proj/weight\n",
      "setting layer 19 mlp/c_proj/bias\n",
      "setting layer 20 ln_1/gamma\n",
      "setting layer 20 ln_1/beta\n",
      "setting layer 20 c_attn/weight\n",
      "setting layer 20 c_attn/bias\n",
      "setting layer 20 c_proj/weight\n",
      "setting layer 20 c_proj/bias\n",
      "setting layer 20 ln_2/gamma\n",
      "setting layer 20 ln_2/beta\n",
      "setting layer 20 mlp/c_fc/weight\n",
      "setting layer 20 mlp/c_fc/bias\n",
      "setting layer 20 mlp/c_proj/weight\n",
      "setting layer 20 mlp/c_proj/bias\n",
      "setting layer 21 ln_1/gamma\n",
      "setting layer 21 ln_1/beta\n",
      "setting layer 21 c_attn/weight\n",
      "setting layer 21 c_attn/bias\n",
      "setting layer 21 c_proj/weight\n",
      "setting layer 21 c_proj/bias\n",
      "setting layer 21 ln_2/gamma\n",
      "setting layer 21 ln_2/beta\n",
      "setting layer 21 mlp/c_fc/weight\n",
      "setting layer 21 mlp/c_fc/bias\n",
      "setting layer 21 mlp/c_proj/weight\n",
      "setting layer 21 mlp/c_proj/bias\n",
      "setting layer 22 ln_1/gamma\n",
      "setting layer 22 ln_1/beta\n",
      "setting layer 22 c_attn/weight\n",
      "setting layer 22 c_attn/bias\n",
      "setting layer 22 c_proj/weight\n",
      "setting layer 22 c_proj/bias\n",
      "setting layer 22 ln_2/gamma\n",
      "setting layer 22 ln_2/beta\n",
      "setting layer 22 mlp/c_fc/weight\n",
      "setting layer 22 mlp/c_fc/bias\n",
      "setting layer 22 mlp/c_proj/weight\n",
      "setting layer 22 mlp/c_proj/bias\n",
      "setting layer 23 ln_1/gamma\n",
      "setting layer 23 ln_1/beta\n",
      "setting layer 23 c_attn/weight\n",
      "setting layer 23 c_attn/bias\n",
      "setting layer 23 c_proj/weight\n",
      "setting layer 23 c_proj/bias\n",
      "setting layer 23 ln_2/gamma\n",
      "setting layer 23 ln_2/beta\n",
      "setting layer 23 mlp/c_fc/weight\n",
      "setting layer 23 mlp/c_fc/bias\n",
      "setting layer 23 mlp/c_proj/weight\n",
      "setting layer 23 mlp/c_proj/bias\n",
      "setting layer 24 ln_1/gamma\n",
      "setting layer 24 ln_1/beta\n",
      "setting layer 24 c_attn/weight\n",
      "setting layer 24 c_attn/bias\n",
      "setting layer 24 c_proj/weight\n",
      "setting layer 24 c_proj/bias\n",
      "setting layer 24 ln_2/gamma\n",
      "setting layer 24 ln_2/beta\n",
      "setting layer 24 mlp/c_fc/weight\n",
      "setting layer 24 mlp/c_fc/bias\n",
      "setting layer 24 mlp/c_proj/weight\n",
      "setting layer 24 mlp/c_proj/bias\n",
      "setting layer 25 ln_1/gamma\n",
      "setting layer 25 ln_1/beta\n",
      "setting layer 25 c_attn/weight\n",
      "setting layer 25 c_attn/bias\n",
      "setting layer 25 c_proj/weight\n",
      "setting layer 25 c_proj/bias\n",
      "setting layer 25 ln_2/gamma\n",
      "setting layer 25 ln_2/beta\n",
      "setting layer 25 mlp/c_fc/weight\n",
      "setting layer 25 mlp/c_fc/bias\n",
      "setting layer 25 mlp/c_proj/weight\n",
      "setting layer 25 mlp/c_proj/bias\n",
      "setting layer 26 ln_1/gamma\n",
      "setting layer 26 ln_1/beta\n",
      "setting layer 26 c_attn/weight\n",
      "setting layer 26 c_attn/bias\n",
      "setting layer 26 c_proj/weight\n",
      "setting layer 26 c_proj/bias\n",
      "setting layer 26 ln_2/gamma\n",
      "setting layer 26 ln_2/beta\n",
      "setting layer 26 mlp/c_fc/weight\n",
      "setting layer 26 mlp/c_fc/bias\n",
      "setting layer 26 mlp/c_proj/weight\n",
      "setting layer 26 mlp/c_proj/bias\n",
      "setting layer 27 ln_1/gamma\n",
      "setting layer 27 ln_1/beta\n",
      "setting layer 27 c_attn/weight\n",
      "setting layer 27 c_attn/bias\n",
      "setting layer 27 c_proj/weight\n",
      "setting layer 27 c_proj/bias\n",
      "setting layer 27 ln_2/gamma\n",
      "setting layer 27 ln_2/beta\n",
      "setting layer 27 mlp/c_fc/weight\n",
      "setting layer 27 mlp/c_fc/bias\n",
      "setting layer 27 mlp/c_proj/weight\n",
      "setting layer 27 mlp/c_proj/bias\n",
      "setting layer 28 ln_1/gamma\n",
      "setting layer 28 ln_1/beta\n",
      "setting layer 28 c_attn/weight\n",
      "setting layer 28 c_attn/bias\n",
      "setting layer 28 c_proj/weight\n",
      "setting layer 28 c_proj/bias\n",
      "setting layer 28 ln_2/gamma\n",
      "setting layer 28 ln_2/beta\n",
      "setting layer 28 mlp/c_fc/weight\n",
      "setting layer 28 mlp/c_fc/bias\n",
      "setting layer 28 mlp/c_proj/weight\n",
      "setting layer 28 mlp/c_proj/bias\n",
      "setting layer 29 ln_1/gamma\n",
      "setting layer 29 ln_1/beta\n",
      "setting layer 29 c_attn/weight\n",
      "setting layer 29 c_attn/bias\n",
      "setting layer 29 c_proj/weight\n",
      "setting layer 29 c_proj/bias\n",
      "setting layer 29 ln_2/gamma\n",
      "setting layer 29 ln_2/beta\n",
      "setting layer 29 mlp/c_fc/weight\n",
      "setting layer 29 mlp/c_fc/bias\n",
      "setting layer 29 mlp/c_proj/weight\n",
      "setting layer 29 mlp/c_proj/bias\n",
      "setting layer 30 ln_1/gamma\n",
      "setting layer 30 ln_1/beta\n",
      "setting layer 30 c_attn/weight\n",
      "setting layer 30 c_attn/bias\n",
      "setting layer 30 c_proj/weight\n",
      "setting layer 30 c_proj/bias\n",
      "setting layer 30 ln_2/gamma\n",
      "setting layer 30 ln_2/beta\n",
      "setting layer 30 mlp/c_fc/weight\n",
      "setting layer 30 mlp/c_fc/bias\n",
      "setting layer 30 mlp/c_proj/weight\n",
      "setting layer 30 mlp/c_proj/bias\n",
      "setting layer 31 ln_1/gamma\n",
      "setting layer 31 ln_1/beta\n",
      "setting layer 31 c_attn/weight\n",
      "setting layer 31 c_attn/bias\n",
      "setting layer 31 c_proj/weight\n",
      "setting layer 31 c_proj/bias\n",
      "setting layer 31 ln_2/gamma\n",
      "setting layer 31 ln_2/beta\n",
      "setting layer 31 mlp/c_fc/weight\n",
      "setting layer 31 mlp/c_fc/bias\n",
      "setting layer 31 mlp/c_proj/weight\n",
      "setting layer 31 mlp/c_proj/bias\n",
      "setting transformer/ln_f/gamma\n",
      "setting transformer/ln_f/beta\n"
     ]
    }
   ],
   "source": [
    "new_weights = []\n",
    "for w in gpt2_model.weights:\n",
    "    num_layer = 0\n",
    "    if 'h_._' in w.name:\n",
    "        num_layer = re.findall(r'transformer/h_._(\\d+)', w.name)[0]\n",
    "\n",
    "    if 'transformer/wte/weight:0' in w.name:\n",
    "        w0 = get_weight(m0, f'word_embeddings.weight')\n",
    "        w1 = get_weight(m1, f'word_embeddings.weight')\n",
    "        w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "        assert w.shape == (30000, 2560)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting transformer/wte/weight')\n",
    "    elif 'transformer/wpe/embeddings:0' in w.name:\n",
    "        w0 = get_weight(m0, f'position_embeddings.weight')\n",
    "        ｗ = w0.numpy()\n",
    "        assert w.shape == (1024, 2560)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting transformer/wpe/embeddings')\n",
    "    elif 'ln_1/gamma:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.input_layernorm.weight')\n",
    "        w = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} ln_1/gamma')\n",
    "    elif 'ln_1/beta:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.input_layernorm.bias')\n",
    "        w = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} ln_1/beta')\n",
    "    elif 'attn/c_attn/weight:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.attention.query_key_value.weight')\n",
    "        w1 = get_weight(m1, f'transformer.layers.{num_layer}.attention.query_key_value.weight')\n",
    "        w0 = w0.numpy()\n",
    "        w1 = w1.numpy()\n",
    "        q0 = w0[:1280, :]\n",
    "        q1 = w1[:1280, :]\n",
    "        k0 = w0[1280:1280 * 2, :]\n",
    "        k1 = w1[1280:1280 * 2, :]\n",
    "        v0 = w0[1280 * 2:, :]\n",
    "        v1 = w1[1280 * 2:, :]\n",
    "        w = np.concatenate([q0, q1, k0, k1, v0, v1])\n",
    "        assert w.shape == (7680, 2560)\n",
    "        w = np.transpose(w)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} c_attn/weight')\n",
    "    elif 'attn/c_attn/bias:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.attention.query_key_value.bias')\n",
    "        w1 = get_weight(m1, f'transformer.layers.{num_layer}.attention.query_key_value.bias')\n",
    "        w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "        w = np.transpose(w)\n",
    "        w = w.reshape(1, 7680)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} c_attn/bias')\n",
    "    elif 'attn/c_proj/weight:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.attention.dense.weight')\n",
    "        w1 = get_weight(m1, f'transformer.layers.{num_layer}.attention.dense.weight')\n",
    "        w = np.concatenate([w0.numpy(), w1.numpy()], axis=-1)\n",
    "        assert w.shape == (2560, 2560)\n",
    "        w = np.transpose(w)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} c_proj/weight')\n",
    "    elif 'attn/c_proj/bias:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.attention.dense.bias')\n",
    "        w = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        w = w.reshape(1, 2560)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} c_proj/bias')\n",
    "    elif 'ln_2/gamma:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.post_attention_layernorm.weight')\n",
    "        w = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} ln_2/gamma')\n",
    "    elif 'ln_2/beta:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.post_attention_layernorm.bias')\n",
    "        w = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} ln_2/beta')\n",
    "    elif 'mlp/c_fc/weight:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.mlp.dense_h_to_4h.weight')\n",
    "        w1 = get_weight(m1, f'transformer.layers.{num_layer}.mlp.dense_h_to_4h.weight')\n",
    "        w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "        assert w.shape == (10240, 2560)\n",
    "        w = np.transpose(w)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} mlp/c_fc/weight')\n",
    "    elif 'mlp/c_fc/bias:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.mlp.dense_h_to_4h.bias')\n",
    "        w1 = get_weight(m1, f'transformer.layers.{num_layer}.mlp.dense_h_to_4h.bias')\n",
    "        w = np.concatenate([w0.numpy(), w1.numpy()])\n",
    "        assert w.shape == (10240, )\n",
    "        w = w.reshape(1, 10240)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} mlp/c_fc/bias')\n",
    "    elif 'mlp/c_proj/weight:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.mlp.dense_4h_to_h.weight')\n",
    "        w1 = get_weight(m1, f'transformer.layers.{num_layer}.mlp.dense_4h_to_h.weight')\n",
    "        w = np.concatenate([w0.numpy(), w1.numpy()], axis=-1)\n",
    "        assert w.shape == (2560, 10240)\n",
    "        w = np.transpose(w)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} mlp/c_proj/weight')\n",
    "    elif 'mlp/c_proj/bias:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.layers.{num_layer}.mlp.dense_4h_to_h.bias')\n",
    "        w = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        w = w.reshape(1, 2560)\n",
    "        new_weights.append(w)\n",
    "        print(f'setting layer {num_layer} mlp/c_proj/bias')\n",
    "    elif 'transformer/ln_f/gamma:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.final_layernorm.weight')\n",
    "        ｗ = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        new_weights.append(w)\n",
    "        print(f'setting transformer/ln_f/gamma')\n",
    "    elif 'transformer/ln_f/beta:0' in w.name:\n",
    "        w0 = get_weight(m0, f'transformer.final_layernorm.bias')\n",
    "        ｗ = w0.numpy()\n",
    "        assert w.shape == (2560, )\n",
    "        new_weights.append(w)\n",
    "        print(f'setting transformer/ln_f/beta')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_tokenizer import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer(\n",
    "    'CPM-Generate/bpe_3w_new/vocab.json',\n",
    "    'CPM-Generate/bpe_3w_new/merges.txt',\n",
    "    model_file='CPM-Generate/bpe_3w_new/chinese_vocab.model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 今天天气不错\n",
      "1 今天天气不错,\n",
      "2 今天天气不错,心情\n",
      "3 今天天气不错,心情也\n",
      "4 今天天气不错,心情也不错\n",
      "5 今天天气不错,心情也不错\n",
      "6 今天天气不错,心情也不错,\n",
      "7 今天天气不错,心情也不错,就\n",
      "8 今天天气不错,心情也不错,就想\n",
      "9 今天天气不错,心情也不错,就想着\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode('今天天气不错')\n",
    "\n",
    "for i in range(10):\n",
    "    output = gpt2_model(tf.constant([ids]))\n",
    "    nid = np.argmax(output[0][0, -1])\n",
    "    ids += [nid]\n",
    "    print(i, tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "encode() got an unexpected keyword argument 'add_special_tokens'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6e44ba419b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_generater\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextGenerationPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpt2_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext_generater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'你好'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# tf_token = GPT2Tokenizer.from_pretrained('./CPM-Generate/bpe_3w_new')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pro/fast-gpt2/env/lib/python3.8/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, return_tensors, return_text, clean_up_tokenization_spaces, prefix, **generate_kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_and_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m                 \u001b[0;31m# set input_ids to None to allow empty prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pro/fast-gpt2/env/lib/python3.8/site-packages/transformers/pipelines.py\u001b[0m in \u001b[0;36m_parse_and_tokenize\u001b[0;34m(self, inputs, padding, add_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mtokenizer_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m         inputs = self.tokenizer(\n\u001b[0m\u001b[1;32m    796\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/pro/fast-gpt2/examples/gpt_tokenizer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: encode() got an unexpected keyword argument 'add_special_tokens'"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer as TFGPT2Tokenizer\n",
    "from transformers import TextGenerationPipeline\n",
    "\n",
    "text_generater = TextGenerationPipeline(gpt2_model, tokenizer)\n",
    "text_generater('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model.save_pretrained('/data2/CPM-TF/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /data2/CPM-TF/models.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'generated_text': '你好 ▁ , ▁我 ▁是 ▁ 个 ▁ <unk> ▁ <unk> ▁ <unk> ▁ <unk> ▁ <unk> ▁ <unk>'}]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFGPT2LMHeadModel\n",
    "from gpt_tokenizer import GPT2Tokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('./CPM-Generate/bpe_3w_new/')\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"/data2/CPM-TF/models\")\n",
    "\n",
    "text_generater = TextGenerationPipeline(gpt2_model, tokenizer)\n",
    "text_generater('你好')"
   ]
  }
 ]
}